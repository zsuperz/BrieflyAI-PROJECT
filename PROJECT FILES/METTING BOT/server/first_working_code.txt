import os
import time
import subprocess
import whisper
import matplotlib.pyplot as plt
from datetime import timedelta
from selenium import webdriver
from selenium.webdriver.edge.service import Service
from selenium.webdriver.edge.options import Options
from selenium.webdriver.common.by import By
from pyannote.audio import Pipeline
from collections import defaultdict
import requests
import json
from dotenv import load_dotenv


load_dotenv()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# Global paths
OUTPUT_AUDIO = "meeting_audio.wav"
OUTPUT_TRANSCRIPT = "meeting_transcript.txt"
SPEAKER_GRAPH_IMAGE = "speaker_chart.png"

# Load models once
print("üîÑ Loading speaker diarization model...")
pipeline = Pipeline.from_pretrained("pyannote/speaker-diarization-3.1")
print("‚úÖ Speaker diarization model loaded.")

print("üîÑ Loading Whisper transcription model...")
whisper_model = whisper.load_model("medium")
print("‚úÖ Whisper model loaded.")

def start_audio_recording():
    print("üéôÔ∏è Starting audio recording using FFmpeg...")
    return subprocess.Popen([
        "ffmpeg",
        "-y",
        "-f", "dshow",
        "-i", 'audio=Stereo Mix (Realtek(R) Audio)',
        OUTPUT_AUDIO
    ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

def stop_audio_recording(process):
    print("üõë Stopping audio recording...")
    process.terminate()
    try:
        process.wait(timeout=5)
    except subprocess.TimeoutExpired:
        process.kill()
    print("‚úÖ Audio recording complete and saved to:", OUTPUT_AUDIO)

def plot_speaker_durations(speaker_durations):
    print("üìä Generating speaker duration bar chart...")
    speakers = list(speaker_durations.keys())
    durations = [speaker_durations[s] for s in speakers]

    plt.figure(figsize=(8, 5))
    plt.bar(speakers, durations, color='skyblue')
    plt.xlabel("Speakers")
    plt.ylabel("Speaking Time (seconds)")
    plt.title("Speaking Time per Speaker")
    plt.tight_layout()
    plt.savefig(SPEAKER_GRAPH_IMAGE)
    plt.close()
    print(f"‚úÖ Speaker duration graph saved to {SPEAKER_GRAPH_IMAGE}")

def diarize_and_transcribe(audio_path):
    print("\nüîç Starting speaker diarization...")
    diarization = pipeline(audio_path)
    print("‚úÖ Diarization complete. Detected speaker segments:")

    segments_with_speakers = []
    speaker_durations = defaultdict(float)

    for segment, _, speaker in diarization.itertracks(yield_label=True):
        duration = segment.end - segment.start
        speaker_durations[speaker] += duration
        print(f"  ‚è±Ô∏è {segment.start:.2f}s - {segment.end:.2f}s: {speaker}")
        segments_with_speakers.append((segment.start, segment.end, speaker))

    plot_speaker_durations(speaker_durations)

    print("\n‚úçÔ∏è Starting Whisper transcription...")
    whisper_result = whisper_model.transcribe(audio_path, verbose=False)
    print("‚úÖ Transcription complete. Matching segments to speakers...")

    def get_speaker_label(timestamp):
        for start, end, speaker in segments_with_speakers:
            if start - 0.5 <= timestamp <= end + 0.5:
                return speaker
        return "Unknown"

    combined_output = []
    for segment in whisper_result['segments']:
        start = segment['start']
        speaker = get_speaker_label(start)
        timestamp = str(timedelta(seconds=int(start)))
        line = f"[{timestamp}] {speaker}: {segment['text']}"
        print(line)
        combined_output.append(line)

    with open(OUTPUT_TRANSCRIPT, "w", encoding="utf-8") as f:
        f.write("\n".join(combined_output))

    print(f"\nüìÑ Transcript with speaker labels saved to: {OUTPUT_TRANSCRIPT}")
    return "\n".join(combined_output)

def force_mute(driver):
    print("üìµ Forcing mic and camera OFF using JavaScript...")
    try:
        driver.execute_script("""
            Array.from(document.querySelectorAll('[data-is-muted]')).forEach(el => {
                if (el.getAttribute('data-is-muted') === 'false') el.click();
            });
        """)
    except Exception as e:
        print("‚ö†Ô∏è JavaScript mute injection failed:", e)



GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")  # Or set directly for testing

def generate_summary_with_gemini(transcript):
    GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")  # or hardcode for testing

    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={GEMINI_API_KEY}"
    headers = {"Content-Type": "application/json"}
    payload = {
        "contents": [
            {
                "parts": [
                    {"text": f"Summarize this meeting:\n\n{transcript}"}
                ]
            }
        ]
    }

    try:
        response = requests.post(url, headers=headers, json=payload)
        response.raise_for_status()
        result = response.json()
        summary = (
            result.get("candidates", [{}])[0]
                  .get("content", {})
                  .get("parts", [{}])[0]
                  .get("text", "No summary available.")
        )
    except Exception as e:
        summary = f"‚ö†Ô∏è Error getting summary: {e}\nFull response:\n{response.text}"

    with open("summary_output.txt", "w", encoding="utf-8") as f:
        f.write(summary)

    return summary







def run_meeting_bot(meeting_url):
    print(f"\nüöÄ Joining meeting at: {meeting_url}")
    
    # Set up Edge options
    options = Options()
    options.add_argument("--use-fake-ui-for-media-stream")
    options.add_argument("--headless")
    options.add_argument("--disable-gpu")
    options.add_argument("user-data-dir=C:/Users/super/AppData/Local/Microsoft/Edge/User Data")
    options.add_argument("profile-directory=Profile 3")
    
    service = Service(executable_path="C:/Users/super/AppData/Local/Microsoft/Microsoft Edge Developer/msedgedriver.exe")
    driver = webdriver.Edge(service=service, options=options)
    driver.get(meeting_url)

    time.sleep(5)

    # Mute mic and camera
    try:
        mic = driver.find_element(By.XPATH, "//div[@role='button'][@aria-label*='microphone']")
        cam = driver.find_element(By.XPATH, "//div[@role='button'][@aria-label*='camera']")
        if mic.get_attribute("aria-pressed") != "false":
            mic.click()
            print("üîá Microphone muted.")
        if cam.get_attribute("aria-pressed") != "false":
            cam.click()
            print("üé• Camera turned off.")
    except Exception as e:
        print("‚ö†Ô∏è Could not toggle mic/camera:", str(e))

    force_mute(driver)
    time.sleep(2)

    # Click join
    try:
        join_btn = driver.find_element(By.XPATH, "//span[contains(text(),'Join now')]/..")
        join_btn.click()
        print("‚úÖ Joined the meeting!")
    except Exception as e:
        print("‚ö†Ô∏è Could not click 'Join now':", str(e))

    # Record audio
    recorder = start_audio_recording()
    try:
        print("üïí Staying in the meeting for 65 seconds...")
        time.sleep(65)
    finally:
        stop_audio_recording(recorder)
        driver.quit()
        print("üëã Left the meeting.")

    # Transcribe & diarize
    transcript_text = diarize_and_transcribe(OUTPUT_AUDIO)

    # Generate summary with Gemini
    summary = generate_summary_with_gemini(transcript_text)

    # Return result to frontend
    return {
        "transcript": transcript_text,
        "chart_url": "/chart",
        "summary": summary
    }


